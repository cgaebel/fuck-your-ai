# Server Configuration
PORT=3000

# ===========================================
# GGUF Model Configuration
# ===========================================

# Path to your GGUF model file
# If not specified, defaults to TinyLlama model
MODEL_PATH=./models/tinyllama-1.1b-chat-v1.0.Q2_K.gguf

# Chat format for specific model families
# Options: gemma, llama, mistral, chatgpt, openai, alpaca, vicuna, chatml, zephyr
# Default: llama (for TinyLlama model)
MODEL_CHAT_FORMAT=llama

# Custom prompt template (advanced)
# Default: "{prompt}"
MODEL_TEMPLATE={prompt}

# Context size for model (default: 2048)
MODEL_CONTEXT_SIZE=2048

# Number of GPU layers to offload (default: 0 = CPU only)
MODEL_GPU_LAYERS=0

# Advanced model parameters (JSON string)
# Example: MODEL_PARAMS={"f16_kv":true,"vocab_only":false}
MODEL_PARAMS={}

# ===========================================
# Common Model Parameters
# ===========================================

# Temperature controls randomness (0.1-1.0)
# Lower = more deterministic, Higher = more creative
MODEL_TEMPERATURE=0.2

# Maximum tokens to generate for each response
MODEL_MAX_TOKENS=600

# Top-p sampling (0.0-1.0)
MODEL_TOP_P=0.7